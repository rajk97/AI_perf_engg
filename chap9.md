Increasing CUDA Kernel Efficiency and Arithmetic Intensity: 

- No. of FLOPS per byte of data transferred. 
- Roofline Model: Kernel Performance(FLOPS/sec) against Arithmetic Intensity (FLOPSs/byte)
- Some improvements: Improve the algorithm, reuse data, fuse operations, and increase batch sizes to raise arithmetic intensity without changing the algorithm's result. 

Multilevel Microtiling and Software Prefetching: 
-  Microtiling is simple -- load tile from global DRAM to shared memory and then do vectorized loads of microtiles into registers using stuff like float4 and <half2>. 

OLD WAY (manual tiling):
  Developer manually moves data at every level:

  DRAM â†’ (you load) â†’ Shared Memory â†’ (you load) â†’ Registers â†’ Tensor Core
                â†‘                            â†‘
          Your code: global load       Your code: float4 load
          coalesced into shmem         from shmem to registers


NEW WAY (Blackwell/modern GPUs):
  Hardware + compiler handle the inner levels:

  DRAM â†’ (you load) â†’ Shared Memory â†’ (hardware moves) â†’ TMEM â†’ Tensor Core
              â†‘                              â†‘
        You still do this            tcgen05 instructions
        (cp.async or TMA)            handle this automatically

WHAT IS TMEM?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  TMEM = Tensor Memory
  A dedicated memory space ONLY Tensor Cores can access
  
  DRAM (big, slow)
    â””â†’ Shared Memory (fast, programmer-visible)
         â””â†’ TMEM (fastest, Tensor Core private, compiler-managed)
              â””â†’ Tensor Core (compute)

  You don't read/write TMEM directly â€” compiler & hardware manage it
  It's like registers, but specifically shaped for matrix fragments

WHAT IS tcgen05?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  tcgen05 = Tensor Core Generation 05 (Blackwell's Tensor Core instructions)
  
  These instructions do TWO things at once:
    1. Move data: Shared Memory â†’ TMEM
    2. Compute: Matrix multiply-accumulate (MMA)
  
  "Implicitly stage" = the instruction handles data movement FOR you
  You just say "multiply these tiles" and hardware fetches from shmem into TMEM


WHAT ARE cp.async AND TMA?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  cp.async = Copy Async: DRAM â†’ Shared Memory without blocking the thread
  TMA = Tensor Memory Accelerator: hardware unit that does DRAM â†’ Shared Memory
        with built-in address calculation (no thread math needed)
  
  Both handle the DRAM â†’ Shared Memory step
  tcgen05 handles the Shared Memory â†’ TMEM step

- Unified memory eases development but may not produce the best performance. Expert users often prefer explicit cudaMemcpy or pinned memory allocations to fully avoid page migration overheads.
- Physical memory is divided into fixed-size CHUNKS called pages.
Typical page size: 4 KB (CPU) or 64 KB (GPU unified memory)


Tiling with Thread Block Clusters: 
- CUDA thread-block clusters from Cooperative Groups allow multiple thread blocks to share data using distributed shared memory -- can be used to batch load data using multiple blocks and TMA for tiling purposes -- using something called multicast. 

WITHOUT CLUSTERS (traditional):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  4 thread blocks each need the SAME tile of matrix A:

  DRAM:  [A tile]
            â”‚
            â”œâ”€â”€â†’ CTA 0 loads A tile into its SMEM  (128 bytes from DRAM)
            â”œâ”€â”€â†’ CTA 1 loads A tile into its SMEM  (128 bytes from DRAM)
            â”œâ”€â”€â†’ CTA 2 loads A tile into its SMEM  (128 bytes from DRAM)
            â””â”€â”€â†’ CTA 3 loads A tile into its SMEM  (128 bytes from DRAM)

  Total DRAM traffic: 4 Ã— 128 = 512 bytes
  Same data loaded 4 times! ğŸ˜¢


WITH CLUSTER (2Ã—2, multicast):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  4 CTAs in a cluster share via DSMEM + TMA multicast:

  DRAM:  [A tile]
            â”‚
            â””â”€â”€â†’ TMA loads ONCE â†’ multicasts to all 4 CTAs' SMEM simultaneously
                      â”‚
                 â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
                 â–¼    â–¼    â–¼    â–¼
               CTA0  CTA1 CTA2 CTA3
               SMEM  SMEM SMEM SMEM

  Total DRAM traffic: 1 Ã— 128 = 128 bytes
  4Ã— reduction! âœ…

- 4 Thread blocks -- 4x the DRAM memory load speed 
- On B200, default is 8, but you can increase the no. of thread blocks per cluster to be 16 -- comes at a cost though 

Kernel Fusion: 
- Fusion = merge multiple kernels into one so intermediates stay in registers, never hit DRAM â†’ higher AI (FLOPs/byte)
- Tradeoff: more fusion = more registers/thread â†’ can reduce occupancy or spill to local memory â€” always profile
- torch.compile / TorchInductor auto-fuses elementwise ops; manual fusion for complex patterns (reductions, norms)
- Vertical fusion = chain sequential ops on same data (sinâ†’sqrt); Horizontal fusion = combine parallel ops across data
- Micro-opt: replace divide with rsqrtf * multiply â€” faster instruction, but only matters if compute-bound
- Rule of thumb: if data is read more than once by same block, stage it in shared memory to kill redundant global loads
- CUTLASS, Triton, TorchInductor help write fused kernels with Tensor Cores + TMA + TMEM

Structured Sparsity: 
