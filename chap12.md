Dynamic Scheduling, CUDA Graphs, and Device-Initiated Kernel Orchestration:

- We'll understand dynamic, device-side, and graph-based kernel orchestration techniques that keep every SM fed across multi-GPU clusters. 
- When work is not evently distributed across thread blocks, SM's will sit idle. 
- Active cycles/Total elapsed SM cycles give an idea of underutilization

- Atomic Counters: 
2/22/26:

- Atomic counters: foundation for dynamic work queues on GPU
- L2 cache services atomics on-chip (~200 cy) â€” fast when uncontended
- Contention: multiple threads hit same address simultaneously â†’ hardware replays â†’ waste
- Nsight Compute metrics:
  - atomic_transactions: total L2 atomic ops (including replays)
  - atomic_transactions_per_request: contention ratio (1.0 = ideal, >1.0 = retries)

- Fix: BATCH the atomic â€” grab N items per atomic instead of 1

  BEFORE (1 item per atomic)          AFTER (32 items per atomic)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  T0: atomicAdd(&head, 1) â”€â”          W0: atomicAdd(&head, 32)
  T1: atomicAdd(&head, 1) â”€â”¤ 32       â”‚ start = result
  T2: atomicAdd(&head, 1) â”€â”¤ atomics  â”‚ __shfl_sync â†’ broadcast to warp
  ...                       â”‚ per      â”‚ all 32 threads process items
  T31: atomicAdd(&head, 1)â”€â”˜ warp     â””â†’ 1 atomic per warp

  Contention: 32Ã—             Contention: 1Ã—

  CODE
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // One thread per warp does the atomic
  int start = atomicAdd(&queue_head, batchSize);   // grab 32 items
  // Broadcast to all lanes
  start = __shfl_sync(0xFFFFFFFF, start, 0);
  // Each thread in warp processes one item
  int myIdx = start + laneId;
  if (myIdx < N) process(data[myIdx]);

  ALTERNATIVE 1: PER-BLOCK COUNTERS
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Global counter      Per-block counters (shared memory)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ head=0  â”‚         â”‚ local=0 â”‚  â”‚ local=0 â”‚  â”‚ local=0 â”‚
  â”‚ (L2)    â”‚         â”‚ (SMEM)  â”‚  â”‚ (SMEM)  â”‚  â”‚ (SMEM)  â”‚
  â”‚         â”‚         â”‚ Block 0 â”‚  â”‚ Block 1 â”‚  â”‚ Block 2 â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚                   â”‚            â”‚            â”‚
  All threads â”€â”€â†’          Threads â”€â”€â†’ SMEM atomic (fast, no L2 contention)
  hammer L2                â”‚            â”‚            â”‚
  (slow)                   â””â”€â”€â”€â”€ periodically merge to global â”€â”€â”€â”€â”˜
                                 (one global atomic per block)

  Flow:
  Thread â†’ atomicAdd(&local_counter, 1)   â† SMEM: ~20 cycles, block-local
  When local_counter hits threshold:
    One thread â†’ atomicAdd(&global_head, threshold)  â† L2: rare, batched
  Result: 1000 threads â†’ ~8 global atomics (1 per block) instead of 1000


  ALTERNATIVE 2: HIERARCHICAL REDUCTION
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Level 0 (thread):     each thread has local count
                        â†“ warp shuffle to sum
  Level 1 (warp):       warp-level total (32â†’1)
                        â†“ SMEM atomic
  Level 2 (block):      block-level total (warpsâ†’1)
                        â†“ global atomic
  Level 3 (global):     final counter in L2

  Thread â”€â”€warp shuffleâ”€â”€â†’ Warp sum â”€â”€SMEM atomicâ”€â”€â†’ Block sum â”€â”€L2 atomicâ”€â”€â†’ Global
  32:1 reduction           8:1 reduction              N:1 reduction
  (registers, free)        (SMEM, ~20 cy)             (L2, ~200 cy)

  Example: 8192 threads (32 blocks Ã— 8 warps Ã— 32 threads)
  Naive:         8192 global atomics
  Per-block:       32 global atomics
  Hierarchical:    32 global atomics, but SMEM atomics also reduced by 8Ã—

  Both alternatives: push contention to cheaper/faster levels of the memory hierarchy

THE PROBLEM: UNEVEN WORK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Static assignment: thread i â†’ item i (fixed)

Warp 0: [==]â”€â”€â”€â”€â”€â”€â”€â”€idleâ”€â”€â”€â”€â”€â”€â”€â”€   fast threads wait for slow ones
Warp 7: [========================] everyone waits for the slowest warp
        â†‘ thread-level imbalance = wasted SIMD lanes (unfixable in SIMT)

THE SOLUTION: DYNAMIC ATOMIC QUEUE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Global counter: [next = 0]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Warp leader: base = atomicAdd(&counter, 32)  â”‚ one atomic per batch
â”‚ 2. __shfl_sync: broadcast base to 32 threads    â”‚ free (register crossbar)
â”‚ 3. Each thread: process(data[base + laneId])     â”‚ all 32 threads work
â”‚ 4. Loop back to step 1 until counter â‰¥ N         â”‚ no idle time
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dynamic result:
Warp 0: [==][==][===][==][====][==]â”€â”€   fast warps grab more batches
Warp 7: [====][===][====][===]â”€â”€â”€â”€â”€â”€â”€   slow warps grab fewer
        â†‘ all warps busy until work exhausted, kernel finishes earlier

WHY IT WORKS â€” THE KEY INSIGHT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Thread-level imbalance:  SIMT lockstep â†’ can't hide â†’ wasted lanes
Warp-level imbalance:    SM warp scheduler â†’ freely hides â†’ no waste

Dynamic queue moves imbalance from thread level â†’ warp level
â†’ where the hardware can absorb it via warp scheduling

CONSTRAINTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Only works for INDEPENDENT items (no data dependencies between items)
- Atomic queue itself adds ~200 cy overhead per batch (L2 atomic)
- Batch size tradeoff: too small = contention, too large = tail imbalance
- 8-32 items per batch is the sweet spot on modern GPUs

GPU MEMORY HIERARCHY FOR COMMUNICATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scope          Via              Latency     Use case
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Threadsâ†’Warp   Warp shuffle     ~0 cycles   Broadcast batch base
Warpsâ†’Block    Shared memory    ~20 cycles  Per-block counters
Blocksâ†’GPU     L2 / Global      ~200 cycles Global work queue  

  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  DYNAMIC SCHEDULING â€” VISUAL DISCOVERY MAP
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  1. THE PROBLEM
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Items:  [light][light][HEAVY][light][HEAVY][light][HEAVY][light]

  Static assignment:
  Block 0 â†’ [light][light]  â†’ done â”€â”€â”€â”€â”€â”€idleâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Block 1 â†’ [HEAVY][light]  â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€doneâ”€â”€â”€â”€â”€â”€â”€â”€
  Block 2 â†’ [HEAVY][light]  â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€doneâ”€â”€â”€â”€â”€â”€â”€â”€â”€
  Block 3 â†’ [HEAVY][light]  â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€doneâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                           â†‘ GPU done here
                                  Block 0 wasted half the time

  2. THE FIX
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Shared queue: [item0|item1|item2|item3|item4|item5|item6|item7]
              â†‘ any warp grabs next available

  Block 0: [grab][grab][grab][grab][grab]â”€â”€queue emptyâ”€â”€EXIT
  Block 1: [  grab  ][  grab  ][ grab  ]â”€â”€queue emptyâ”€â”€EXIT
                                       â†‘ GPU done here (earlier)

  3. HOW: while(true) + atomicAdd
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Static kernel body:              Dynamic kernel body:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ get fixed idx  â”‚               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ compute        â”‚               â”‚ â”‚ atomicAdd â†’ base â”‚ â”‚
  â”‚ return (die)   â”‚               â”‚ â”‚ shfl â†’ broadcast â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚ â”‚ compute          â”‚ â”‚
                                â”‚ â”‚ done? loop back  â”‚ â”‚
                                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                â”‚ queue empty? â†’ returnâ”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  4. THREAD vs WARP IMBALANCE
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Thread imbalance (UNFIXABLE â€” SIMT lockstep):
  â”Œâ”€â”€â”€ Warp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ T0:  doneâ”€â”€â”€â”€â”€idleâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â† can't do anything else
  â”‚ T31: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€doneâ”€â”€â”€â”€â”‚ â† everyone waits for this
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Warp imbalance (FIXABLE â€” warp scheduler hides it):
  SM warp scheduler picks whichever warp is ready:
  â†’ Warp 0 done? grab more work
  â†’ Warp 1 stalled? skip, run Warp 2
  â†’ Warp 2 done? grab more work

  5. THE QUEUE MECHANISM
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  globalIndex in L2:  [0] â†’ [32] â†’ [64] â†’ ... â†’ [N] STOP

  Warp leader â”€â”€atomicAdd(&globalIndex, 32)â”€â”€â†’ gets base
    â”‚
  __shfl_sync â”€â”€broadcast baseâ”€â”€â†’ all 32 threads know
    â”‚
  T0: base+0    T1: base+1    ...    T31: base+31
    â”‚              â”‚                     â”‚
  process()      process()             process()
    â”‚
  loop back â”€â”€â†’ next atomicAdd

  6. WHY IT WORKS
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Static:                        Dynamic:
  total time = max(block)        total time = total_work / num_SMs

    â–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† slowest        â–ˆâ–ˆ  â† all finish together
    â–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       â–ˆâ–ˆ
    â–ˆâ–ˆâ–ˆâ–ˆ                           â–ˆâ–ˆ

  7. CONSTRAINTS
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Works:   item[i] independent of item[j]    (histogram, map, element-wise)
  âœ— Breaks:  item[i] depends on item[i-1]      (prefix sum, recurrence)

  Queue overhead:  ~200 cy per atomicAdd (L2)
  Batch size:      8-32 = sweet spot (less = contention, more = tail waste)

is it also safe to say that thread level imbalance still exists but maybe minimized due to small batch sizes and hidden with warp scheduling here? but the main alpha comes from dynamic work instead? please tell, thank you

Thread imbalance still exists â€” SIMT lockstep is physics, can't eliminate it
Minimized â€” batch of 32 consecutive items means work = idx%256 varies by only 31 across the warp (e.g., 64â†’95 instead of 0â†’255), so the spread is smaller than random assignment
Hidden â€” warp scheduler switches to other warps while the slow warp finishes, so the SM doesn't stall
But yes, the main alpha is the dynamic work distribution â€” blocks/warps never idle because they keep pulling from the queue. Thread-level gains are a nice side effect, not the point.

Mnemonic: Thread imbalance = minor tax (small spread + scheduler hides it), dynamic queue = the real win.

  WITHOUT CUDA GRAPHS â€” CPU launches kernels one by one
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  CPU:  [==kernel launch==]      [==kernel launch==]      [==kernel launch==]
  GPU:                     [kern]                   [kern]                   [kern]
                        â†‘ gap â†‘                  â†‘ gap â†‘                  â†‘ gap
                   CPU overhead              CPU overhead             CPU overhead
                   between each              between each             between each

  GPU is IDLE while CPU prepares each launch â†’ short kernels = mostly waiting


  WITH CUDA GRAPHS â€” one graph launch dispatches all kernels
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  CPU:  [==graph launch==]
  GPU:                     [kern][kern][kern]
                        â†‘ back-to-back, no gaps

  CPU does ONE call â†’ GPU runs entire pipeline without waiting for CPU between ops
  The shorter the kernels, the bigger the win (less gap overhead per kernel)

  CUDA GRAPHS
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - Capture an entire pipeline (kernels, copies, events, callbacks) into a DAG once
  - Replay it every iteration with one CPU call â†’ near-zero CPU launch overhead
  - Dependencies are baked into the graph â†’ no manual sync between ops
  - Does NOT magically overlap more than streams already do â€” it eliminates the
 CPU-side cost of enqueuing each op individually every iteration

PyTorch, Inference Engines, and CUDA Graphs:

  - PyTorch: torch.cuda.Graph captures static op sequences; reduce-overhead compiler mode
 auto-wraps eligible segments in graphs (not guaranteed for all paths, may increase memory)
  - vLLM / TensorRT-LLM: pre-capture one graph per batch size at startup
 â†’ bucket/pad inputs to match a captured graph's fixed shape â†’ replay at runtime
  - Constraint: graphs need STATIC shapes (fixed tensor addresses, fixed sizes)
 â†’ dynamic shapes = multiple graphs (one per bucket) or fall back to eager mode

 MEMORY POOLS FOR CUDA GRAPHS â€” SUMMARY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

- Pre-allocate memory outside the graph, not inside
- Frameworks like PyTorch use static memory pools per graph
- Graph can overlap independent transfers and compute (like streams)
  but won't make any single op faster â€” it eliminates CPU scheduling overhead
- Dependency graph known upfront = automatic overlap without manual sync

YOUR INSIGHTS (confirmed correct)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. "the memory pool basically has same addresses coz its like fixed
    memory and hence graphs with memory operations inside of them
    with fixed memory addresses can safely use this memory pools"

2. "the main alpha here is the blocks instead of carrying a fixed
    amount of work carries a dynamic amount of work"
    (from dynamic scheduling â€” same principle applies: graphs want
    fixed/static things, dynamic things go outside)

3. "instead of block, its the kernel being an atomic queue is the
    real dynamic scheduling implementation here"
    (and graphs are the opposite â€” they want everything STATIC
    and predictable, which is why dynamic ops go in eager mode)

CUDA GRAPHS: Core Concept
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WITHOUT CUDA Graph:                    WITH CUDA Graph:
  CPU          GPU                      CPU          GPU
   â”‚                                     â”‚
   â”œâ”€launch Aâ”€â”€â†’ [A]                     â”œâ”€captureâ”€â”€â†’ record A,B,C
   â”œâ”€launch Bâ”€â”€â†’    [B]                  â”‚            (one-time setup)
   â”œâ”€launch Câ”€â”€â†’       [C]              â”‚
   â”‚                                     â”œâ”€replayâ”€â”€â”€â†’ [A][B][C]
   3 CPUâ†’GPU round trips                 â”œâ”€replayâ”€â”€â”€â†’ [A][B][C]
   per iteration                         â”œâ”€replayâ”€â”€â”€â†’ [A][B][C]
                                         1 CPU call per iteration!

- CUDA Graph = record a sequence of GPU ops once, replay with ONE launch call â€” eliminates per-kernel CPUâ†’GPU launch overhead
- Capture: BeginCapture â†’ enqueue kernels/copies/events â†’ EndCapture â†’ Instantiate â†’ Replay in loop
- Only works for REPETITIVE sequences (same ops, same shapes, same pointers each iteration)
- cudaGraphExecUpdate: tweak node parameters (pointers, sizes) without recapturing entire graph
- Partial capture OK: only the repetitive portion needs to be graphed
- PyTorch: torch.cuda.CUDAGraph() + with torch.cuda.graph(g): block to capture, g.replay() to launch
- Gotcha: must use static buffers (fixed memory addresses) â€” no allocations inside capture
- Gotcha: warm-up pass required before capture to initialize lazy CUDA/cuBLAS/cuDNN setup
- Mnemonic: "Record once, replay forever â€” CUDA Graph removes the CPU middleman."

- CUDA Graphs: 300 kernel launches â†’ 100 graph replays, ~25% faster end-to-end by eliminating CPUâ†’GPU handshakes
- Zero GPU idle time between kernels (back-to-back execution) vs ~3Âµs gaps without graphs
- Pitfall: graph is invalid if workload size changes â€” must recapture or use cudaGraphExecUpdate
- Pitfall: no memory allocation or host-device sync inside capture â€” allocate everything before capture
- Pointer stability: all tensors must stay at same memory address across replays â€” no reallocation between iterations
- PyTorch: torch.cuda.graph_pool_handle() creates dedicated memory pool for fixed-address tensors during capture
- Update inputs by copying into static tensors (static_x.copy_(new_data)), never reallocate
- No host-side ops inside capture: print(), RNG, nested captures, memory allocation â€” graph must be pure deterministic GPU work
- All tensors pre-allocated with fixed addresses + fixed shapes before capture â€” no resizing or cudaMalloc during capture

Data updating between replays??

SETUP (once):
  static_input  = torch.empty(shape, device='cuda')  â† fixed address
  static_output = torch.empty(shape, device='cuda')  â† fixed address

CAPTURE (once):
  with torch.cuda.graph(g):
      # Graph records: "read from static_input's ADDRESS, write to static_output's ADDRESS"
      static_output = model(static_input)

REPLAY LOOP (every iteration):
  for batch in dataloader:
      static_input.copy_(batch)    â† Copy NEW data into the SAME address
      g.replay()                   â† Graph runs on whatever is at that address now
      result = static_output       â† Read result from fixed address

Dynamic Graph Update: 

- Dynamic update = change kernel parameters, grid/block dims, or pointers in an existing graph without recapturing (~few Âµs)
- Workflow: capture template graph at MAX expected size â†’ update parameters per iteration for actual size
- Can change: scalar values, pointers, grid dims, swap kernel of same signature
- Cannot change: graph structure (add/remove nodes) â†’ must recapture if topology changes
- Best for: semi-static workloads (same pipeline, few varying parameters like batch size)
- Mnemonic: "Capture the skeleton once, update the muscles each iteration."

Device-Initiated CUDA Graph Launch:
- GPU can launch graphs automatically
- Device-initiated launch = GPU kernel triggers a prerecorded graph entirely on-device, no CPU round-trip
- Setup: capture graph on host â†’ cudaGraphInstantiate with DeviceLaunch flag â†’ cudaGraphUpload to GPU memory (must upload before device launch)
- ~2Ã— lower launch latency vs host-side graph launch, and latency stays flat regardless of graph size/width
- Host-launch latency grows with more nodes/branches (CPU scheduling overhead); device-launch does not
- Launch API from device code: cudaGraphLaunch(graphExec, stream) with special stream values for mode selection
- Fire-and-forget (cudaStreamGraphFireAndForget): child graph runs immediately, concurrently with parent kernel (max 120 per graph execution)
- Tail launch (cudaStreamGraphTailLaunch): child runs after parent finishes
- Sibling launch (cudaStreamGraphFireAndForgetAsSibling): concurrent like fire-and-forget but scheduled as sibling
- Debug with Nsight Systems (child graphs appear as separate GPU timeline streams) + NVTX markers around cudaGraphLaunch calls

- Self-relaunch pattern: scheduler kernel calls cudaGetCurrentGraphExec() to get its own graph handle, then tail-launches itself
- This creates an infinite GPU-resident loop: schedule â†’ work â†’ schedule â†’ work... with zero CPU involvement
- Only 1 pending self-tail-launch allowed at a time (prevents unbounded self-queuing / stack overflow on GPU)
- Tail launches from the SAME graph execute in enqueue order (FIFO) â€” so worker runs before self-relaunch
- Tail launches from NESTED graphs (child enqueuing more tails) run BEFORE the parent's remaining tails (LIFO/stack insertion)
- Max 255 pending tail launches total per graph
- Sibling launch = fire-and-forget but runs as a peer (not child) of parent graph, in parent's stream environment
- Key difference from fire-and-forget: sibling doesn't delay parent's tail launches (fire-and-forget child could)
- API: cudaGraphLaunch(graphExec, cudaStreamGraphFireAndForgetAsSibling)
- Decision guide: need results from graph â†’ tail launch; side task â†’ fire-and-forget/sibling; independent peer work â†’ sibling
- Practical pattern: GPU kernel branches on data content and directly launches the appropriate prerecorded graph (e.g., LZ vs Huffman) â€” no CPU round-trip to decide

FIRE-AND-FORGET          TAIL LAUNCH              SIBLING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Parent â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€â”€â†’    Parent â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        Parent â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€â”€â†’
       â†“                        â†“                        â†“
Child  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (concurrent)   (parent done)     Sibling â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (concurrent)
       (is a child)            â†“                        (is a peer)
                         Graph â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 
                         (runs after parent)     Tail launches NOT blocked
                                                 by sibling

Atomic Queues and Device-Initiated CUDA Graphs for
In-Kernel Persistent Scheduling:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PERSISTENT SCHEDULER KERNEL (loops on GPU)          â”‚
    â”‚                                                      â”‚
    â”‚   while (work in queue) {                            â”‚
    â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
    â”‚       â”‚ idx = atomicAdd(&queueHead, 1)       â”‚       â”‚
    â”‚       â”‚        (claim next token)            â”‚       â”‚
    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
    â”‚                          â–¼                           â”‚
    â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
    â”‚       â”‚ TAIL-LAUNCH: Decode Graph            â”‚       â”‚
    â”‚       â”‚ (precaptured attention + FFN)        â”‚       â”‚
    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
    â”‚                          â–¼                           â”‚
    â”‚       (graph completes â†’ loop back)                  â”‚
    â”‚   }                                                  â”‚
    â”‚                                                      â”‚
    â”‚   CPU: ğŸ˜´ not involved                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Conditional Graph Nodes:
    
- Conditional graph nodes = embed control flow (IF, IF/ELSE, WHILE, SWITCH) directly into CUDA Graphs, evaluated on GPU
- Controlled by a "condition handle" â€” a small integer set on-device via cudaGraphSetConditional(handle, flag)
- IF: runs body once if flag â‰  0 | IF/ELSE: two body graphs, picks one | WHILE: loops body while flag â‰  0 | SWITCH: N body graphs, runs the i-th one
- Setup: create handle â†’ add upstream kernel that computes & sets handle â†’ add conditional node â†’ populate body subgraph(s)
- Set condition from a single thread to avoid races; ensure memory is flushed before conditional node reads it
- Conditional nodes can nest (e.g., WHILE body contains IF) for multilevel logic â€” all on GPU
- PyTorch has no Python API for conditional nodes yet â€” requires custom C++ integration

Dynamic Parallelism: Launch child kernels from the parent kernel based on the evalution of the parent kernel without using CPU resources -- consumes more GPU resources though. 

- Dynamic Parallelism (DP) = parent kernel spawns child kernels on GPU at runtime â€” no prerecorded graph needed
- Use case: irregular/data-dependent workloads where execution shape isn't known ahead of time (adaptive mesh, graph traversal, recursive problems)
- Diagnostic signal: profiler shows Kernel A â†’ GPU idle gap â†’ Kernel B (idle = CPU deciding next launch)
- Parent kernel doesn't complete until all its children complete (implicit sync, no cudaDeviceSynchronize needed)
- Launch from single thread (threadIdx.x==0 && blockIdx.x==0) to avoid duplicate child launches
- Compile with -rdc=true (relocatable device code)
- Costs: per-launch overhead (~25Âµs vs ~20Âµs host-side), extra stack space, max 2048 pending child launches (configurable via cudaLimitDevRuntimePendingLaunchCount)
- Bump stack if needed: cudaDeviceSetLimit(cudaLimitStackSize, newSize)
- Example result: 3 host launches â†’ 1 host launch, ~40% idle â†’ ~5% idle, ~25% faster overall
- Bonus: data locality preserved â€” intermediate results stay on GPU, no cache eviction from CPU round-trips
- DP is NOT always a win â€” if child kernel work is too small, device-side launch overhead negates the benefit. Always profile with Nsight Compute.

Decision guide:

CUDA Graphs          â†’ execution shape known ahead of time, repeated many times (amortize capture cost)
Conditional Nodes    â†’ known structure with runtime branching (IF/WHILE/SWITCH)
Device-Initiated Graph â†’ known graphs, GPU decides WHICH to launch
Dynamic Parallelism  â†’ execution shape unknown, emerges from data at runtime

Graphs for plans, DP for surprises

Orchestrate Acrsoss Multiple GPUs and Cluster Nodes(NVSHMEM):

- Node = one physical machine in a cluster (e.g., one DGX with 8 GPUs)
- Inter-node = across machines over InfiniBand (GPUDirect RDMA)
- Intra-node = within one machine over NVLink/NVSwitch (GPUDirect P2P)
- NUMA node is a different concept â€” refers to CPU socket + local memory affinity within one machine

- Core goal at multi-GPU scale: hide data movement behind compute (overlap is mandatory, not optional)
- Even NVLink is slower than on-device HBM â€” so P2P transfers MUST overlap with compute on separate streams
- P2P point-to-point: cudaMemcpyPeerAsync on a comm stream (copy engines, zero SM cost)
- Collectives (all-reduce): NCCL on a separate stream â€” auto-arranges rings/trees to saturate NVLink/NVSwitch
- Inter-node: CUDA-aware MPI auto-uses GPUDirect RDMA over InfiniBand (no CPU staging)
- Intra-node: GPUDirect P2P over NVLink or PCIe
- Pattern: stream 1 = compute, stream 2 = communication â€” both run concurrently

Fine-Grained GPU-to-GPU Memory Sharing with NVSHMEM: 

- NVSHMEM = GPU-level shared memory library using PGAS (Partitioned Global Address Space) â€” each GPU is a "PE" (processing element)
- Key capability: GPU kernel directly reads/writes another GPU's memory from device code, no CPU involved
- Pattern: put data â†’ nvshmem_quiet() (ensure completion) â†’ set flag on remote GPU; receiver spins on flag then reads data
- All communication is one-sided RMA (Remote Memory Access) â€” single hardware transaction over NVLink/PCIe
- vs cudaMemcpyPeerAsync: that's CPU-initiated (host tells copy engine); NVSHMEM is GPU-initiated (kernel code does the write)
- Achieves near-peak wire speed by bypassing CPU + kernel launch overhead
- Danger: it's GPU-level shared-memory programming â€” you own synchronization and race prevention
- Prefer fine-grained signals (nvshmem_wait_until, nvshmem_signal_*) over global barriers (nvshmem_barrier_all) to avoid stalling all GPUs on the slowest peer

- NVSHMEM kernels can be captured inside CUDA Graphs (operations are inside kernels, not separate graph nodes)
- Transformer pipeline example: GPU 0 computes attention â†’ NVSHMEM puts activations to GPU 1 + signals â†’ GPU 1 starts MLP; GPU 0 immediately starts next batch â†’ both GPUs in perfect tandem, near-100% utilization
- Work-stealing pattern: nvshmem_int_atomic_inc on a global queue counter â€” each GPU grabs next task dynamically, no host coordination
- Cooperative launch: nvshmemx_collective_launch() starts same kernel on ALL GPUs simultaneously â€” required for device-side barriers/collectives
- nvshmem_barrier_all() inside cooperative kernel = lockstep execution across all GPUs (use sparingly â€” stalls on slowest GPU)

Capturing Multi-GPU Collectives with NCCL and CUDA Graphs:
- Without this, there is overhead of host nccl calls everytie -- instead put on a graph and keep replaying. 
- Capture forward + ncclAllReduce + backward into one CUDA Graph â†’ replay with single cudaGraphLaunch per iteration
- All ranks MUST capture and replay the same NCCL sequence with the same communicator (mismatch â†’ deadlock or silent corruption)
- Bucketed all-reduce: split gradients into chunks on streamA, all-reduce each chunk on streamB â†’ compute/comms overlap hides network time
- PyTorch DDP does bucketed all-reduce automatically; wrapping in CUDA Graph further reduces CPU overhead + gives deterministic timing
- Practical rules: allocate gradient/comm buffers BEFORE capture; warm up collectives first; use cudaGraphExecUpdate to patch params without recapture
- At scale (100K+ GPUs), per-step CPU savings compound â†’ tighter sync, higher utilization across cluster
- NCCL = bulk collectives (all-reduce, broadcast). NVSHMEM = fine-grained, async, point-to-point GPUâ†”GPU sharing

Pattern for N-GPU Scaling:
- Scaling pattern is always the same: dispatch once, overlap transfers with compute, keep CPU off critical path
- Ideal: N GPUs = NÃ— speedup â€” only if all GPUs stay fed with data and communication is hidden behind compute
- Without overlap, scaling plateaus when comms time = compute time (Amdahl's law on communication)
- As GPU count grows: more aggressive pipelining, less CPU orchestration â€” offload to async copies, NCCL-in-graphs, NVSHMEM

Roofline-Guided Scheduling and Orchestration Decisions:
- Memory-bound kernel: kernel fusion helps modestly (fewer global mem round-trips); real gains from latency masking + more loads/stores in flight + reduce precision (FP16/FP8/FP4)
- Compute-bound kernel: fusion shines (combine ops â†’ higher FLOPS/byte â†’ shift right on roofline); reduce launch overhead with persistent kernels, CUDA Graphs, device-side launches to keep ALUs fed
- Neither bound (middle of roofline): increase concurrency â€” parallel streams, concurrent graphs, multiple persistent kernels â†’ aggregate throughput climbs on both axes
- Persistent kernels, graphs, device-initiated launches don't change arithmetic intensity â€” they reduce idle gaps, pushing actual perf closer to the ceiling
- Always: Nsight Compute to measure FLOPS + bytes â†’ plot on roofline â†’ pick strategy based on which roof you're under â†’ measure again after each change

ROOFLINE MODEL:
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ compute ceiling (peak FLOPS)
  Performance             â”‚
  (FLOPS/s)     â”€â”€â”€â”€â”€â”€â”€â”€â”€/
               /         â”‚
              / â†slope   â”‚
             /  (memory  â”‚
            /   ceiling) â”‚
           /             â”‚
          /              â”‚
         /               â”‚
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
                  Arithmetic Intensity (FLOPS/byte)


CASE 1: Below the slope
  Ã—  â† your kernel (below the line)
  /
  You're NOT hitting peak memory bandwidth
  = memory-INEFFICIENT (bad access patterns, uncoalesced, misaligned)
  Fix: coalesce, align, SoA, vectorized loads


CASE 2: On the slope
  /
  Ã— â† your kernel (on the line)
  You ARE hitting peak memory bandwidth
  = memory-BOUND (bandwidth wall reached)
  Can't squeeze more bandwidth, BUT you can:
    â†’ Shift RIGHT: reduce precision (FP16â†’FP8), fuse ops, reuse data in registers/smem
    â†’ This increases FLOPS/byte â†’ moves you toward the flat (compute) ceiling


CASE 3: Below the flat ceiling
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Ã— â† your kernel (below the flat part)
  = compute-INEFFICIENT (low occupancy, idle gaps, launch overhead)
  Fix: persistent kernels, CUDA Graphs, increase occupancy


CASE 4: On the flat ceiling
         â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€
  = compute-BOUND (ALUs maxed out)
  Only fix: better algorithm or faster hardware

Chapter 12 done!