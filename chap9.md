Increasing CUDA Kernel Efficiency and Arithmetic Intensity: 

- No. of FLOPS per byte of data transferred. 
- Roofline Model: Kernel Performance(FLOPS/sec) against Arithmetic Intensity (FLOPSs/byte)
- Some improvements: Improve the algorithm, reuse data, fuse operations, and increase batch sizes to raise arithmetic intensity without changing the algorithm's result. 

Multilevel Microtiling and Software Prefetching: 
-  Microtiling is simple -- load tile from global DRAM to shared memory and then do vectorized loads of microtiles into registers using stuff like float4 and <half2>. 

OLD WAY (manual tiling):
  Developer manually moves data at every level:

  DRAM â†’ (you load) â†’ Shared Memory â†’ (you load) â†’ Registers â†’ Tensor Core
                â†‘                            â†‘
          Your code: global load       Your code: float4 load
          coalesced into shmem         from shmem to registers


NEW WAY (Blackwell/modern GPUs):
  Hardware + compiler handle the inner levels:

  DRAM â†’ (you load) â†’ Shared Memory â†’ (hardware moves) â†’ TMEM â†’ Tensor Core
              â†‘                              â†‘
        You still do this            tcgen05 instructions
        (cp.async or TMA)            handle this automatically

WHAT IS TMEM?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  TMEM = Tensor Memory
  A dedicated memory space ONLY Tensor Cores can access
  
  DRAM (big, slow)
    â””â†’ Shared Memory (fast, programmer-visible)
         â””â†’ TMEM (fastest, Tensor Core private, compiler-managed)
              â””â†’ Tensor Core (compute)

  You don't read/write TMEM directly â€” compiler & hardware manage it
  It's like registers, but specifically shaped for matrix fragments

WHAT IS tcgen05?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  tcgen05 = Tensor Core Generation 05 (Blackwell's Tensor Core instructions)
  
  These instructions do TWO things at once:
    1. Move data: Shared Memory â†’ TMEM
    2. Compute: Matrix multiply-accumulate (MMA)
  
  "Implicitly stage" = the instruction handles data movement FOR you
  You just say "multiply these tiles" and hardware fetches from shmem into TMEM


WHAT ARE cp.async AND TMA?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  cp.async = Copy Async: DRAM â†’ Shared Memory without blocking the thread
  TMA = Tensor Memory Accelerator: hardware unit that does DRAM â†’ Shared Memory
        with built-in address calculation (no thread math needed)
  
  Both handle the DRAM â†’ Shared Memory step
  tcgen05 handles the Shared Memory â†’ TMEM step

- Unified memory eases development but may not produce the best performance. Expert users often prefer explicit cudaMemcpy or pinned memory allocations to fully avoid page migration overheads.
- Physical memory is divided into fixed-size CHUNKS called pages.
Typical page size: 4 KB (CPU) or 64 KB (GPU unified memory)


Tiling with Thread Block Clusters: 
- CUDA thread-block clusters from Cooperative Groups allow multiple thread blocks to share data using distributed shared memory -- can be used to batch load data using multiple blocks and TMA for tiling purposes -- using something called multicast. 

WITHOUT CLUSTERS (traditional):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  4 thread blocks each need the SAME tile of matrix A:

  DRAM:  [A tile]
            â”‚
            â”œâ”€â”€â†’ CTA 0 loads A tile into its SMEM  (128 bytes from DRAM)
            â”œâ”€â”€â†’ CTA 1 loads A tile into its SMEM  (128 bytes from DRAM)
            â”œâ”€â”€â†’ CTA 2 loads A tile into its SMEM  (128 bytes from DRAM)
            â””â”€â”€â†’ CTA 3 loads A tile into its SMEM  (128 bytes from DRAM)

  Total DRAM traffic: 4 Ã— 128 = 512 bytes
  Same data loaded 4 times! ğŸ˜¢


WITH CLUSTER (2Ã—2, multicast):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  4 CTAs in a cluster share via DSMEM + TMA multicast:

  DRAM:  [A tile]
            â”‚
            â””â”€â”€â†’ TMA loads ONCE â†’ multicasts to all 4 CTAs' SMEM simultaneously
                      â”‚
                 â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
                 â–¼    â–¼    â–¼    â–¼
               CTA0  CTA1 CTA2 CTA3
               SMEM  SMEM SMEM SMEM

  Total DRAM traffic: 1 Ã— 128 = 128 bytes
  4Ã— reduction! âœ…

- 4 Thread blocks -- 4x the DRAM memory load speed 
- On B200, default is 8, but you can increase the no. of thread blocks per cluster to be 16 -- comes at a cost though 

Kernel Fusion: 
- Fusion = merge multiple kernels into one so intermediates stay in registers, never hit DRAM â†’ higher AI (FLOPs/byte)
- Tradeoff: more fusion = more registers/thread â†’ can reduce occupancy or spill to local memory â€” always profile
- torch.compile / TorchInductor auto-fuses elementwise ops; manual fusion for complex patterns (reductions, norms)
- Vertical fusion = chain sequential ops on same data (sinâ†’sqrt); Horizontal fusion = combine parallel ops across data
- Micro-opt: replace divide with rsqrtf * multiply â€” faster instruction, but only matters if compute-bound
- Rule of thumb: if data is read more than once by same block, stage it in shared memory to kill redundant global loads
- CUTLASS, Triton, TorchInductor help write fused kernels with Tensor Cores + TMA + TMEM

Structured Sparsity: 
- 2:4 structured sparsity = exactly 2 of every 4 weights are zero â†’ hardware skips zeros, doubles Tensor Core throughput
- Applied post-training (pruning) for inference only â€” training gradients don't benefit
- Sparse Tensor Cores operate on half-width data, doing 2Ã— work per cycle on nonzero elements
- Increases AI by cutting memory traffic ~50% (don't load zeros) while keeping same useful FLOPs
- PyTorch: to_sparse_semi_structured() converts dense â†’ 2:4 sparse format for Sparse Tensor Cores
- Needs large matmuls + large batches to amortize index/compression overhead â€” small batches see less benefit
- Only 2:4 pattern is hardware-accelerated â€” arbitrary sparsity gets no special acceleration
- Apply AFTER basic optimizations (coalescing, tiling, fusion) are already in place

Recomputation vs. Memory Trade-Off: 
- Calculate x^2 twice instead of storing and reading it as memory is expensive. 

PyTorch and Arithmetic Intensity: 
- PyTorch auto-fuses elementwise ops via torch.compile and uses cuDNN/cuBLAS for tiled matmuls â€” you get tiling, fusion, shared memory for free
- SDPA dispatches to FlashAttention/cuDNN automatically â€” control with sdpa_kernel(SDPBackend.FLASH_ATTENTION)
- Prefer high-level ops (torch.matmul, nn.functional) over many small kernels â€” libraries call optimized fused kernels
- Custom/nonstandard ops may not get auto-fused â€” manual optimization still needed for those

2/15/26: 
- On variable length sequences, use PyTorch's nested/rugged tensors
- Batch of 3 sequences, lengths: [3, 5, 2]

With padding (standard approach, pad to max_len=5):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ seq1: [A B C 0 0]         â”‚  â† 2 zeros = wasted memory loads
â”‚ seq2: [D E F G H]         â”‚  â† full, no waste
â”‚ seq3: [I J 0 0 0]         â”‚  â† 3 zeros = wasted memory loads
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total: 15 elements loaded, only 10 are useful = 33% waste
Attention computes on zeros too = wasted FLOPS

With Nested/Ragged Tensors â€” Pack Tightly:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [A B C D E F G H I J]  â† one contiguous buffer, NO zeros
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+ metadata: offsets = [0, 3, 8, 10]  â† "seq1 starts at 0, seq2 at 3, seq3 at 8"

- Especially valuable for LLM attention with mixed sequence lengths (e.g., batch has lengths [32, 512, 128]).
- Still maturing in PyTorch â€” verify operator coverage for your workload and profile both memory traffic and kernel time.

Mixed Precision and Utilizing Tensor Cores:
- NVIDIA GPU's have TF32, FP16, FP8, FP4, INT8 in Tensor Cores 
- Mixed precision solves memory-bound primarily because smaller datatypes = fewer bytes moved for the same FLOPS = higher AI.
- Tensor Cores raise the compute roof (more peak FLOPS), TMA+TMEM feed them without stalls â€” together they make it nearly impossible to stay memory-bound.

Blackwell SM:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Register File (256 KB)              â”‚ â† general purpose: local vars, addresses, loop counters
  â”‚                                      â”‚
  â”‚  TMEM (256 KB)                       â”‚ â† dedicated: Tensor Core accumulators ONLY
  â”‚                                      â”‚
  â”‚  Shared Memory (228 KB)              â”‚ â† shared across block: tiles, communication
  â”‚                                      â”‚
  â”‚  CUDA Cores (ALUs)                   â”‚ â† scalar FP32/INT ops
  â”‚  Tensor Cores                        â”‚ â† matrix ops (MMA)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Inside one SM:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  4 Warp Schedulers                  â”‚
  â”‚     â”‚                               â”‚
  â”‚     â”œâ”€â”€â†’ CUDA Cores (128 per SM)    â”‚  â† scalar ops: add, multiply, compare
  â”‚     â”‚    one thread = one operation  â”‚     e.g., x = a + b
  â”‚     â”‚                               â”‚
  â”‚     â”œâ”€â”€â†’ Tensor Cores (4 per SM)    â”‚  â† matrix ops: 16Ã—16 MMA in one instruction
  â”‚     â”‚    one warp = one matrix op   â”‚     e.g., D = AÃ—B + C (whole tile at once)
  â”‚     â”‚                               â”‚
  â”‚     â”œâ”€â”€â†’ Load/Store Units           â”‚
  â”‚     â””â”€â”€â†’ Special Function Units     â”‚  â† sin, cos, sqrt, etc.
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

- Each Blackwell SM has 256 KB TMEM dedicated to Tensor Core accumulators â€” frees registers for other work, boosts occupancy.
- TMA asynchronously copies tiles from global â†’ shared memory, Tensor Core instructions implicitly move data from shared memory â†’ TMEM â€” no SM intervention needed.
- Lower precision (FP16/FP8/FP4) = fewer bytes per element = higher arithmetic intensity for same FLOPS = shifts kernel from memory-bound toward compute-bound.
- AI boost is multiplicative: FP16 = 2Ã— AI, FP8 = 4Ã— AI, FP4 = 8Ã— AI compared to FP32.
- In Nsight Compute, successful mixed precision shows: memory stalls drop, dependency/pipeline stalls rise â€” that means you shifted the bottleneck from memory to compute.
- Verify with Nsight Compute Roofline chart (AI should shift right) and Speed of Light panel (memory util down, compute util up).

- CUTLASS = CUDA Templates for Linear Algebra Subroutines. It's NVIDIA's open-source C++ template library for writing high-performance GEMM (matrix multiply) and convolution kernels.

Without CUTLASS:
  You manually write: TMA loads â†’ shared mem staging â†’ TMEM allocation â†’ PTX MMA instructions â†’ pipeline sync
  = hundreds of lines of expert-level CUDA

With CUTLASS:
  You configure a template: "I want FP16 GEMM, 128Ã—128 tiles, 2-stage pipeline"
  CUTLASS generates the optimized kernel with TMA + TMEM + Tensor Core instructions automatically

- It's what cuBLAS uses under the hood. PyTorch â†’ cuBLAS/cuDNN â†’ CUTLASS-level code â†’ Tensor Cores.
- CUDA Pipeline API (<cuda/pipeline>) = formal producer/consumer sync for overlapping async memory copies with Tensor Core compute.
- cuda::memcpy_async copies global â†’ shared memory without blocking the SM â€” the SM can compute on previous tiles while the next tile loads.
- Double buffering (ping-pong): two shared memory buffers â€” load into one while computing from the other â†’ memory latency hidden behind compute.
- Tensor Cores work without pipelining, but without it the SM stalls waiting for data â€” pipelining is what makes them run at full throughput.
- You don't manually manage TMEM â€” hardware and libraries (CUTLASS) handle operand movement between shared memory and TMEM automatically.

TF32 and Automatic Mixed Precision(PyTorch):

  - TF32 = FP32 exponent (8-bit range) + FP16 mantissa (10-bit precision) â†’ runs on Tensor Cores, not CUDA cores â†’ higher TFLOPS, same dynamic range as FP32.
  - torch.set_float32_matmul_precision('high') â†’ torch.matmul silently routes FP32 inputs through TF32 Tensor Cores.
  - AMP autocast: GEMM/conv â†’ FP16/BF16 Tensor Cores, accumulation â†’ FP32, sensitive ops (layernorm, softmax) â†’ FP32.
  - BF16 exponent = 8 bits (same as FP32) â†’ no overflow â†’ no GradScaler needed. FP16 exponent = 5 bits â†’ overflow risk â†’ needs GradScaler.
  - Format cheat sheet:
      FP32: 1 sign + 8 exp + 23 mantissa = 4 bytes
      TF32: 1 sign + 8 exp + 10 mantissa = 19 bits (internal only, stored as FP32)
      BF16: 1 sign + 8 exp + 7 mantissa = 2 bytes
      FP16: 1 sign + 5 exp + 10 mantissa = 2 bytes
  - Throughput ladder: FP32 CUDA cores < TF32 Tensor Cores < BF16/FP16 Tensor Cores < FP8 < FP4 (each step ~2Ã— TFLOPS).

- When using reduced precision or 2:4 sparsity, use higher batch sizes to amortize overheads and maximize Tensor Core utilization.

BF16/FP16, FP8, and FP4 Reduced Precision:
- BF16/FP16 on Tensor Cores â‰ˆ 4Ã— FP32 throughput â€” Tensor Cores issue many FMAs per cycle on half-sized elements.
- FP16 exponent = 5 bits â†’ small gradients underflow to zero â†’ needs loss scaling (static or dynamic GradScaler).
- BF16 exponent = 8 bits (same as FP32) â†’ no underflow â†’ no loss scaling needed â†’ simpler training workflow.
- BF16 preferred for training on modern GPUs â€” FP32-like stability without GradScaler complexity.
- Loss scaling = multiply loss by a large factor before backward pass â†’ keeps tiny gradients above FP16's underflow threshold â†’ unscale after.

- BF16 has 4x FLOPS/s compared to FP32
Think of the MMA as:  A[MÃ—K] Ã— B[KÃ—N] = C[MÃ—N]

FP32 (TF32):   M=16, K=8,  N=8   â†’ 16Ã—8 Ã— 8Ã—8  = 1024 FMAs
FP16:           M=16, K=16, N=16  â†’ 16Ã—16 Ã— 16Ã—16 = 4096 FMAs
                     â†‘       â†‘
                   K doubled  N doubled  (2Ã— Ã— 2Ã— = 4Ã—)

                        FP32          FP16
Bytes per element:      4             2          â†’ 2Ã— fewer bytes
FLOPS (same algorithm): same          same       â†’ same useful work
AI:                     F/B           F/(B/2)    â†’ 2Ã— AI

Peak TFLOPS (hardware): 80T           300T+      â†’ ~4Ã— peak throughput

- FP8 = half the bytes of FP16 â†’ 2Ã— weights per HBM transaction â†’ 2-3Ã— BF16/FP16 TFLOPS with FP32/TF32 accumulation.
- NVFP4 = 4-bit format with two-level micro-scaling (per-microblock + higher-level scale) to retain accuracy at extreme compression.
- B200 NVFP4 peak = 10 PFLOPS vs FP32 peak = 80 TFLOPS â†’ ~125Ã— theoretical throughput gain per weight.
- B300 (Ultra) = 15 PFLOPS NVFP4 â†’ 50% more than B200.
- FP4 elements are tiny â†’ 256 KB TMEM fits huge tiles (256Ã—256) â†’ more on-chip reuse â†’ less DRAM traffic.
- Low-precision accumulation happens automatically: kernel reads FP4 from HBM â†’ Tensor Cores do FP4Ã—FP4 â†’ accumulate into FP32/BF16.
- Each precision drop (FP32â†’FP16â†’FP8â†’FP4) roughly doubles ops/byte â†’ doubles AI â†’ pushes kernel from memory-bound â†’ compute-bound.
- Accuracy must be validated per model â€” NVFP4 needs calibration to confirm precision drop is acceptable.

- Smaller precision = more elements per fixed-width datapath = more FMAs per Tensor Core instruction = higher TFLOPS.
- Smaller precision = fewer bytes per element = more data per HBM transaction = higher AI.
- Accumulation stays in FP32 because summing thousands of small products in low precision loses significant digits.

INT8 Reduced Precision and DP4A Instructions for Inference: 
- INT8 = 1 byte/element vs FP32 = 4 bytes â†’ 75% less memory traffic for weights â†’ 4Ã— more data per HBM transaction.
- DP4A = SIMD dot-product on CUDA cores: 4 INT8 MACs per instruction vs 1 FP32 FMA â†’ 4Ã— throughput even on regular cores.
- INT8 also runs on Tensor Cores via integer MMA instructions â†’ even higher throughput than DP4A on CUDA cores.
- Primarily for inference â€” training needs higher precision for gradients.
- TMA + TMEM keep INT8 data feeding Tensor Cores without stalls â†’ compute-bound, not memory-bound.

Using CUTLASS for Optimal Arithmetic Intensity and Tensor Core Performance:
- "CUTLASS is a C++ template library that generates high-performance Tensor Core kernels from a configuration â€” you specify WHAT you want (precision, tile size, pipeline), it builds HOW to do it."
- CUTLASS does automatic optimizations like sharedâ€memory tiling, asynchronous memory transfers, and double buffering. 
- All you have to do is to make sure your kernel uses CUTLASS optimally
GEMM: General Matrix Multiply

Without cp.async (old way):

DRAM â†’ L2 â†’ L1 â†’ Registers â†’ Registers â†’ Shared Memory
                   â†‘            â†‘     â†‘
              cache hit    load into   store from
                          register    register to SMEM

Thread does: register = global_load(addr);   // data lands in register
             shared_mem[idx] = register;      // thread writes to SMEM

 With cp.async (bypass L1):

DRAM â†’ L2 â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Shared Memory  -- using DMA not TMA
             skips L1, skips registers 

cp.async:  thread issues the copy instruction (still uses thread to set up address/size)
TMA:       dedicated hardware unit does everything â€” address calc, copy, no thread involvement at all
           DRAM â†’ L2 â†’ Shared Memory (hardware handles it entirely)                        

Method              Who sets up the copy?     Who moves the data?      Thread busy?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Regular load/store  Thread (address + load)   Thread (via registers)   YES â€” fully blocked
cp.async            Thread (issues instruction) Hardware DMA engine     Partially â€” thread issues cmd, then free
TMA                 Hardware unit             Hardware unit             NO â€” thread just triggers it, done

- cp.async - DMA 
- cp.async.bulk.tensor - TMA
- CUTLASS also leverages thread block clusters

Hand-tuned MMA kernel:
  - Written by an expert in raw PTX/CUDA
  - Manually manages TMA descriptors, TMEM allocation, pipeline stages, barrier sync
  - Hundreds of lines of low-level code
  - Squeezes every last drop of performance

CUTLASS GEMM kernel:
  - Generated from C++ templates
  - You configure: tile size, precision, pipeline depth, TMA policy
  - CUTLASS generates the kernel with all the same optimizations
  - Much less code, much easier to maintain

Hand-tuned MMA:   ~98-100% Tensor Core utilization
CUTLASS GEMM:     ~95-98% Tensor Core utilization
                   â†‘
                  "a few percent" difference

CUTLAS subset of CUBLAS subset of PyTorch

Agent: Iteratively decide if CUTLASS is sufficient or you generate an optimized MMA kernel yourself. 


